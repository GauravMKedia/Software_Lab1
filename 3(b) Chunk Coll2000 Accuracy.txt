3(b)  Evaluate the test.txt data from conll2000 corpus after chunking(accuracy,precision,recall,f-measure).

import nltk
from nltk.corpus import conll2000

# Download the conll2000 corpus if not already downloaded
nltk.download('conll2000')

# Load the test data from conll2000 corpus
test_data = conll2000.chunked_sents('test.txt', chunk_types=['NP'])

# Load the chunk parser based on the conll2000 chunking data
chunk_parser = nltk.RegexpParser("NP: {<DT>?<JJ>*<NN.*>+}")

# Initialize counters for evaluation metrics
true_positives = 0
false_positives = 0
false_negatives = 0

# Iterate over the test data and perform chunking
for sentence in test_data:
    sentence_tokens = nltk.tree2conlltags(sentence)
    sentence_pos_tags = [pos for (token, pos, chunk) in sentence_tokens]
    sentence_chunks = chunk_parser.parse(sentence_pos_tags)

    # Extract the chunks from the parsed sentence
    extracted_chunks = set([chunk for chunk in nltk.chunk.tree2conlltags(sentence_chunks) if chunk != 'O'])

    # Compare the extracted chunks with the gold-standard chunks
    gold_chunks = set([chunk for (token, pos, chunk) in sentence_tokens if chunk != 'O'])

    true_positives += len(extracted_chunks & gold_chunks)
    false_positives += len(extracted_chunks - gold_chunks)
    false_negatives += len(gold_chunks - extracted_chunks)

# Calculate evaluation metrics
accuracy = (true_positives / (true_positives + false_positives + false_negatives)) * 100
precision = (true_positives / (true_positives + false_positives)) * 100
recall = (true_positives / (true_positives + false_negatives)) * 100
f_measure = (2 * precision * recall) / (precision + recall)

# Print the evaluation metrics
print("Evaluation Metrics:")
print("Accuracy: {:.2f}%".format(accuracy))
